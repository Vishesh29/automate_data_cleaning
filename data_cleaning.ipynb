{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929ba922-fcff-4334-a5f0-c5c51c1018dd",
   "metadata": {},
   "source": [
    "## Fully Automate Data Cleaning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dad405-3bfe-470b-b182-d5f7a7321e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f1434-0bb3-4210-9d82-e1d1f9d96679",
   "metadata": {},
   "source": [
    "### Step 1: Run Basic Data Quality Checks\n",
    " \n",
    "We need to identify:\n",
    "- Missing values in each column\n",
    "- Duplicate rows\n",
    "- Basic data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2194faa-4ab9-46b0-8d34-d26b798ed45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df):\n",
    "    # Store initial data quality metrics\n",
    "    quality_report = {\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'total_rows': len(df),\n",
    "        'memory_usage': df.memory_usage().sum() / 1024**2  # in MB\n",
    "    }\n",
    "    return quality_report\n",
    "\n",
    "# This gives us a baseline understanding of our data's quality and helps identify the specific cleaning tasks we'll need to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b1897-c631-4fc6-94c6-a75b52c976be",
   "metadata": {},
   "source": [
    "### Step 2: Standardize Data Types\n",
    "\n",
    "We ensure all fields have the right/expected data types. This includes:\n",
    "\n",
    "- Converting string dates to datetime objects\n",
    "- Identifying and converting numeric strings to actual numbers\n",
    "- Ensuring categorical variables are properly encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5365047f-2e30-40da-a2b6-32126d038af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_datatypes(df):\n",
    "    for column in df.columns:\n",
    "        # Try converting string dates to datetime\n",
    "        if df[column].dtype == 'object':\n",
    "            try:\n",
    "                df[column] = pd.to_datetime(df[column])\n",
    "                print(f\"Converted {column} to datetime\")\n",
    "            except ValueError:\n",
    "                # Try converting to numeric if datetime fails\n",
    "                try:\n",
    "                    df[column] = pd.to_numeric(df[column].str.replace('$', '').str.replace(',', ''))\n",
    "                    print(f\"Converted {column} to numeric\")\n",
    "                except:\n",
    "                    pass\n",
    "    return df\n",
    "\n",
    "# This step prevents type-related errors in subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02b3c0-ff00-48ee-8ab9-86bc4900a52a",
   "metadata": {},
   "source": [
    "### Step 3: Handle Missing Values\n",
    "Rather than dropping data records with missing values, we can use imputation strategies:\n",
    "\n",
    "- Using median imputation for numeric columns\n",
    "- Applying mode imputation for categorical data\n",
    "- Maintaining the statistical properties of the dataset while filling gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0c7191-eeb7-4911-951e-5df7139e6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    # Handle numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df[numeric_columns] = num_imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_columns) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df[categorical_columns] = cat_imputer.fit_transform(df[categorical_columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c70d1-c018-48c9-9168-e28d3b32ac40",
   "metadata": {},
   "source": [
    "### Step 4: Detect and Handle Outliers\n",
    "\n",
    "Here's an approach using the Interquartile Range (IQR) method:\n",
    "\n",
    "- Calculate Interquartile Range (IQR) for numeric columns\n",
    "- Identify values beyond 1.5 * IQR from quartiles\n",
    "- Apply capping to extreme values rather than removing them\n",
    "\n",
    "This preserves data while managing extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d33d106-c13c-4a9b-9217-c77b960a61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    outliers_removed = {}\n",
    "    \n",
    "    for column in numeric_columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Count outliers before removing\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].shape[0]\n",
    "        \n",
    "        # Cap the values instead of removing them\n",
    "        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        if outliers > 0:\n",
    "            outliers_removed[column] = outliers\n",
    "            \n",
    "    return df, outliers_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863aed7-4689-4e5c-ab0e-bd8835268c48",
   "metadata": {},
   "source": [
    "### Step 5: Validate the Results\n",
    "\n",
    "After cleaning, we need to verify that our pipeline worked as expected:\n",
    "\n",
    "- Confirm no remaining missing values\n",
    "- Check for any remaining duplicates\n",
    "- Validate data integrity and consistency\n",
    "- Generate a comprehensive cleaning report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c6a867-db24-4121-b378-73a4fda921c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cleaning(df, original_shape, cleaning_report):\n",
    "    validation_results = {\n",
    "        'rows_remaining': len(df),\n",
    "        'missing_values_remaining': df.isnull().sum().sum(),\n",
    "        'duplicates_remaining': df.duplicated().sum(),\n",
    "        'data_loss_percentage': (1 - len(df)/original_shape[0]) * 100\n",
    "    }\n",
    "    \n",
    "    # Add validation results to the cleaning report\n",
    "    cleaning_report['validation'] = validation_results\n",
    "    return cleaning_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e50bb0-f693-421b-9ee1-161c228b951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put everything in complete pipeline\n",
    "def automated_cleaning_pipeline(df):\n",
    "    # Store original shape for reporting\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    # Initialize cleaning report\n",
    "    cleaning_report = {}\n",
    "    \n",
    "    # Execute each step and collect metrics\n",
    "    cleaning_report['initial_quality'] = check_data_quality(df)\n",
    "    \n",
    "    df = standardize_datatypes(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df, outliers = remove_outliers(df)\n",
    "    cleaning_report['outliers_removed'] = outliers\n",
    "    \n",
    "    # Validate and finalize report\n",
    "    cleaning_report = validate_cleaning(df, original_shape, cleaning_report)\n",
    "    \n",
    "    return df, cleaning_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ad4789-c69a-4f6a-84bd-3cb95b03fdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       " 0                  5.1               3.5                1.4               0.2   \n",
       " 1                  4.9               3.0                1.4               0.2   \n",
       " 2                  4.7               3.2                1.3               0.2   \n",
       " 3                  4.6               3.1                1.5               0.2   \n",
       " 4                  5.0               3.6                1.4               0.2   \n",
       " ..                 ...               ...                ...               ...   \n",
       " 145                6.7               3.0                5.2               2.3   \n",
       " 146                6.3               2.5                5.0               1.9   \n",
       " 147                6.5               3.0                5.2               2.0   \n",
       " 148                6.2               3.4                5.4               2.3   \n",
       " 149                5.9               3.0                5.1               1.8   \n",
       " \n",
       "      Species  \n",
       " 0        0.0  \n",
       " 1        0.0  \n",
       " 2        0.0  \n",
       " 3        0.0  \n",
       " 4        0.0  \n",
       " ..       ...  \n",
       " 145      2.0  \n",
       " 146      2.0  \n",
       " 147      2.0  \n",
       " 148      2.0  \n",
       " 149      2.0  \n",
       " \n",
       " [150 rows x 5 columns],\n",
       " {'initial_quality': {'missing_values': {'sepal length (cm)': 0,\n",
       "    'sepal width (cm)': 0,\n",
       "    'petal length (cm)': 0,\n",
       "    'petal width (cm)': 0,\n",
       "    'Species': 0},\n",
       "   'duplicates': np.int64(1),\n",
       "   'total_rows': 150,\n",
       "   'memory_usage': np.float64(0.005847930908203125)},\n",
       "  'outliers_removed': {'sepal width (cm)': 4},\n",
       "  'validation': {'rows_remaining': 150,\n",
       "   'missing_values_remaining': np.int64(0),\n",
       "   'duplicates_remaining': np.int64(1),\n",
       "   'data_loss_percentage': 0.0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test the pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "dataframe = load_iris()\n",
    "dataframe = pd.DataFrame(data= np.c_[dataframe['data'], dataframe['target']],\n",
    "                  columns= dataframe['feature_names'] + ['Species'])\n",
    "\n",
    "automated_cleaning_pipeline(dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
